{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1606.03657 InfoGAN \n",
    "\n",
    "## Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\n",
    "\n",
    "### OpenAI\n",
    "### UC Berkeley, Department of Electrical Engineering and Computer Sciences\n",
    "url :  https://arxiv.org/abs/1606.03657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary by Song GH__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 URL : \n",
    "- 초짜 대학원생의 입장에서 이해하는 InfoGAN (1) : http://jaejunyoo.blogspot.com/2017/03/infogan-1.html\n",
    "- [학부생의 딥러닝] GANs | InfoGAN : Information maximizing GAN : https://haawron.tistory.com/10\n",
    "- InfoGAN Review : https://kangbk0120.github.io/articles/2017-08/info-gan\n",
    "- https://www.slideshare.net/ssuser06e0c5/infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets-72268213\n",
    "- https://github.com/sungreong/Infogan\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영상 URL : \n",
    "    \n",
    "- 차준범: PR-022: InfoGAN (OpenAI)  : https://www.youtube.com/watch?v=_4jbgniqt_Q\n",
    "- https://www.youtube.com/watch?v=r3L3JT_TLTM\n",
    "- https://www.youtube.com/watch?v=ohRtxx30Ev8\n",
    "- https://www.youtube.com/watch?v=-PWYPaAESTM\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List\n",
    "0. Abstract\n",
    "1. Introduction\n",
    "2. Related Work\n",
    "3. Background: Generative Adversarial Networks\n",
    "4. Mutual Information for Inducing Latent Codes\n",
    "5. Variational Mutual Information Maximization\n",
    "6. Implementaion\n",
    "7. Experiments\n",
    " - 7.1 Mutual Information Maximization\n",
    " - 7.2 Disentangled Representation\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CG\n",
    "A. Proof of Lemma 5.1<br>\n",
    "B. interpretation as \"Sleep-Sleep\" Algorithm<br>\n",
    "C. Experiment Setup<br>\n",
    " - C.1 MNIST\n",
    " - C.2 SVHN\n",
    " - C.3 CelebA\n",
    " - C.4 Faces\n",
    " - C.5 Chairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Abstract\n",
    "\n",
    "- <font color='red'>InfoGAN is a generative adversarial network </font>that also maximizes the mutual information between a small subset of the latent variables and the observation.\n",
    "- We derive a lower bound of the mutual information objective that can be optimized efficiently.\n",
    "- successfully disentangles writing styles ... also discovers visual concepts ...\n",
    "- <font color='blue'>Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'></font>\n",
    "<font color='blue'></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "### 1 paragraph\n",
    "- <font color='blue'>A popular framework for unsupervised learning is that of</font>  __representation learning__ [1, 2], whose goal is to use unlabelled data to learn a representation that exposes important semantic features as easily decodable factors.\n",
    "- ... to be useful for many downstream tasks which include classification, regression, visualization, and policy learning in reinforcement learning.\n",
    "\n",
    "### 2 paragraph\n",
    "- ... __a disentangled representation__, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks.\n",
    "- A disentangled representation can be useful for natural tasks that require knowledge of the salient attributes of the data, which include tasks like face recognition and object recognition.\n",
    "- Thus, to be useful, an unsupervised learning algorithm must in effect correctly guess the likely set of downstream classification tasks without being directly exposed to them.\n",
    "\n",
    "### 3 paragraph\n",
    "- A significant fraction of unsupervised learning research is driven by generative modelling.\n",
    "- <font color='blue'>it is hoped that a good generative model will automatically learn a disentangled representation,</font>\n",
    "- The most prominent generative models are the __variational autoencoder (VAE) [3] and the generative adversarial network (GAN)__\n",
    "\n",
    "\n",
    "### 4 paragraph - Most Important Paragraph - Almost all of context\n",
    "- <font color='red'>In this paper, we present a simple modification to the generative adversarial network objective that encourages it to learn interpretable and meaningful representations.</font>\n",
    "- We do so __by maximizing the mutual information between a fixed small subset of the GAN’s noise variables and the observations__, which turns out to be relatively straightforward.\n",
    "- __Despite its simplicity, we found our method to be surprisingly effective: it was able to discover highly semantic and meaningful hidden representations__ on a number of image datasets:...\n",
    "- __The quality of our unsupervised disentangled representation matches previous works that made use of supervised label information__ [5–9]. \n",
    "- <font color='blue'>These results suggest that generative modelling augmented with a mutual information cost could be a fruitful approach for learning disentangled representations.</font>\n",
    "\n",
    "\n",
    "### 5 paragraph - How to wrote the paper order\n",
    "- we begin with a review of the related work ... review GANs ... \n",
    "- We describe how maximizing mutual information results in interpretable representations and derive a simple and efficient algorithm for doing so.\n",
    "- Finally, ... compare InfoGAN ... show that InfoGAN can learn interpretable representations on complex datasets where no previous unsupervised approach is known to learn representations of comparable quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Background : Generative Adversarial Networks\n",
    "\n",
    "$$min_G max_D V(D,G) = E_{x \\sim P_{data}}[logD(x)] + E_{z \\sim noise}[log(1-D(G(z))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Mutual Information for Inducing Latent Codes\n",
    "\n",
    "### 1 Paragraph\n",
    "- The GAN formulation ... the noise will be used by the generator in a highly entangled way, causing the individual dimensions of z to not correspond to semantic features of the data.\n",
    "\n",
    "### 2 Paragraph\n",
    "- However, many domains naturally decompose into a set of semantically meaningful factors of variation.\n",
    "- ... it would be ideal if the model automatically \n",
    " - chose to allocate a discrete random variable to represent the numerical identity of the digit (0-9),\n",
    " - and chose to have two additional continuous variables that represent the digit’s angle and thickness of the digit’s stroke\n",
    "- ... by simply specifying that an MNIST digit is generated by an independent 1-of-10 variable and two independent continuous variables.\n",
    "\n",
    "### 3 Paragraph\n",
    "- In this paper, ... we propose to decompose the input noise vector into two parts:\n",
    " - (i) z, which is treated as source of incompressible noise;\n",
    " - (ii) c, which we will call the latent code and will target the salient structured semantic features of the data distribution.\n",
    " \n",
    "### 5 Paragraph\n",
    "- we propose an information-theoretic regularization: \n",
    " - there should be high mutual information between latent codes c and generator distribution G(z, c).\n",
    " - Thus $I(c;G(z, c))$ should be high.\n",
    " \n",
    "### 6 Paragraph\n",
    "- In information theory, mutual information between X and Y ,\n",
    " - $I(X;Y)$, \n",
    " - measures the “amount of information” learned from knowledge of random variable Y about the other random variable X.\n",
    "\n",
    "$$I(X;Y) = H(X) − H(X|Y) = H(Y) − H(Y|X)\\cdot\\cdot\\cdot (2)$$ \n",
    "\n",
    "### 7 Paragraph\n",
    "- $I(X;Y)$ is the reduction of uncertainty in X when Y is observed.\n",
    "- If X and Y are independent, then I(X; Y ) = 0, \n",
    " - because knowing one variable reveals nothing about the other;\n",
    "- if X and Y are related by a deterministic, invertible function, then maximal mutual information is attained.\n",
    "- given any $x \\sim P_G(x)$, we want $P_G(c|x)$ to have a small entropy.\n",
    " - In other words, the information in the latent code c should not be lost in the generation process.\n",
    "\n",
    "$$min_G max_D V_I(D,G) = V(D,G) − \\lambda I(c;G(z, c)) \\cdot\\cdot\\cdot (3)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Variational Mutual Information Maximization\n",
    "\n",
    "\n",
    "- we can obtain a lower bound of it by defining an auxiliary distribution $Q(c|x)$ to approximate $P(c|x)$:\n",
    "<img src='./1606.03657 InfoGAN/infogan 4.png'>\n",
    "- This technique of lower bounding mutual information is known as <font color='blue'>__Variational Information Maximization__</font>\n",
    "- ... in this paper we opt for __simplicity__ by fixing the latent code distribution and <font color='red'> __we will treat H(c) as a constant__</font>\n",
    "- ... but we still need to be able to sample from the posterior in the inner expectation \n",
    "- Next we state a simple lemma ... that removes the need to sample from the posterior.\n",
    "\n",
    "<img src='./1606.03657 InfoGAN/infogan 7.png'>\n",
    "<img src='./1606.03657 InfoGAN/infogan 5.png'>\n",
    "\n",
    "- We note that $L_I(G,Q)$ is easy to approximate with Monte Carlo simulation. \n",
    "- In particular, $L_I$ can be maximized w.r.t. Q directly and w.r.t. G via the reparametrization trick. \n",
    "- <font color='red'>Hence $L_I(G,Q)$ can be added to GAN’s objectives with no change to GAN’s training procedure and we call the resulting algorithm Information Maximizing Generative Adversarial Networks (InfoGAN).  </font>\n",
    "\n",
    "$$\\cdots$$\n",
    "\n",
    "- Hence, InfoGAN is defined as the following minimax game with a variational regularization of mutual information and a hyperparameter $\\lambda$:\n",
    "\n",
    "$$\\color{Brown}{min_{G,Q}max_D V_{InfoGAN}(D,G,Q) = V(D,G) - \\lambda L_I(G,Q)\\cdots (6)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implementation\n",
    "\n",
    "### 1 paragragh\n",
    "- In practice, we parametrize the auxiliary distribution Q as a neural network\n",
    "- In most experiments, Q and D share all convolutional layers and there is one final fully connected layer to output parameters for the conditional distribution $Q(c|x)$, which means <font color='red'>__InfoGAN only adds a negligible computation cost to GAN.__</font>\n",
    "- We have also observed that <font color='blue'>__$L_I(G,Q)$ always converges faster than normal GAN objectives__ </font>\n",
    "\n",
    "### 2 Paragraph\n",
    "- __For categorical latent code $c_i$__, we use the natural choice of softmax nonlinearity to represent $Q(c_i|x)$.\n",
    "- __For continuous latent code $c_j$__, there are more options depending on what is the true posterior $P(c_j|x)$.\n",
    "- In our experiments, we have found that simply treating $Q(c_j|x)$ as a factored __Gaussian is sufficient.__\n",
    "\n",
    "### 3 Paragraph\n",
    "- ... $\\lambda$ ... <b>1</b> is sufficient for __discrete latent codes.__\n",
    "- When the latent code contains __continuous variables, a smaller $\\lambda$ is typically used__ to ensure that $\\lambda L_I(G,Q)$, which now involves differential entropy, is on the same scale as GAN objectives.\n",
    "\n",
    "### 4 Paragraph\n",
    "- we design our experiments based on ... __DC-GAN, which are enough to stabilize InfoGAN training__ ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Experiments\n",
    "\n",
    "- __The first goal of our experiments is to investigate if mutual information can be maximized efficiently.__\n",
    "- __The second goal is to evaluate if InfoGAN can learn disentangled and interpretable representations by making use of the generator to vary only one latent factor at a time__ in order to assess if varying such factor results in only one type of semantic variation in generated images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Mutual Information Maximization\n",
    "\n",
    "<img width=300 src='./1606.03657 InfoGAN/infogan figure1.png'>\n",
    "\n",
    "### 1 Paragraph\n",
    "- we train InfoGAN on MNIST dataset with a uniform categorical distribution on latent codes c ~ Cat(K = 10, p = 0.1).\n",
    "- <font color='red'>the lower bound $L_I(G,Q)$ is quickly maximized to $H(c)\\simeq$  2.30, which means the bound (4) is tight and maximal mutual information is achieved.</font>\n",
    "\n",
    "### 2 Paragraph\n",
    "- we also train a regular GAN with an auxiliary distribution Q ... \n",
    "- hence there is little mutual information between latent codes and generated images in regular GAN.\n",
    "- This comparison is meant to demonstrate that in a regular GAN, there is no guarantee that the generator will make use of the latent codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Disentangled Representation\n",
    "\n",
    "<img width=300 src='./1606.03657 InfoGAN/infogan figure2.png'>\n",
    "### 1 Paragraph\n",
    "- latent codes with one categorical code, c1 ~ Cat(K = 10, p = 0.1), ... two continuous codes c2,c3 ~ Unif(−1, 1).\n",
    "\n",
    "### 2 Paragraph\n",
    "- we show that the discrete code c1 captures drastic change in shape.\n",
    "- ... even if we just train InfoGAN without any label, c1 can be used as a classifier that achieves 5% error rate\n",
    "\n",
    "### 3 Paragraph\n",
    "\n",
    "- __Continuous codes c2, c3__ capture continuous variations in style: __c2 models rotation of digits and c3 controls the width.__\n",
    "- What is remarkable is that in both cases, the generator does not simply stretch or rotate the digits but instead adjust other details like thickness or stroke style to make sure the resulting images are natural looking.\n",
    "- we manipulated the latent codes in an exaggerated way: instead of plotting latent codes from −1 to 1, we plot it from −2 to 2\n",
    "\n",
    "<img src='./1606.03657 InfoGAN/infogan figure3.png'>\n",
    "<img src='./1606.03657 InfoGAN/infogan figure4.png'>\n",
    "\n",
    "\n",
    "### 4,5,6 Paragraph\n",
    "- we evaluate InfoGAN on two datasets of 3D images: faces and chairs\n",
    "- On the faces dataset, DC-IGN learns to represent latent factors as __azimuth (pose), elevation, and lighting as continuous latent variables by using supervision.__\n",
    "- five continuous codes, $c_i \\sim Unif(−1, 1) \\quad with \\quad 1 \\le i \\le 5$\n",
    "- InfoGAN is able to discover such variation on its own: for instance in Figure 3d latent code that smoothly changes a face from wide to narrow is learned even though this variation was neither explicitly generated or labeled in prior work.\n",
    "\n",
    "### 7 Paragraph\n",
    "- On the chairs dataset, DC-IGN can learn a continuous code that representes rotation.\n",
    "- four categorical codes, c1, c2, c3, c4 ~ Cat(K = 20, p = 0.05) and one continuous code c5 ~ Unif(−1, 1).\n",
    "\n",
    "<img src='./1606.03657 InfoGAN/infogan figure5.png'>\n",
    "<img src='./1606.03657 InfoGAN/infogan figure6.png'>\n",
    "\n",
    "### 8 Paragraph\n",
    "- Next we evaluate InfoGAN on the Street View House Number (SVHN) dataset\n",
    "- we make use of four 10−dimensional categorical variables and two uniform continuous variables as latent codes.\n",
    "\n",
    "### 9 Paragraph\n",
    "- Finally we show ... on another challenging dataset: CelebA\n",
    "- InfoGAN can disentangle other highly semantic variations like presence or absence of glasses, hairstyles and emotion ... without any supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion\n",
    "\n",
    "- This paper introduces a representation learning algorithm called __Information Maximizing Generative Adversarial Networks (InfoGAN).__\n",
    "- <font color='red'>__InfoGAN is completely unsupervised and learns interpretable and disentangled representations on challenging datasets.__</font>\n",
    "- <font color='red'>__In addition, InfoGAN adds only negligible computation cost on top of GAN and is easy to train.__</font>\n",
    "- The core idea of using mutual information to induce representation can be applied to other methods like VAE, which is a promising area of future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Experiment Setup\n",
    "\n",
    "- For all experiments, we use Adam for online optimization and apply batch normalization\n",
    "- We use an up-convolutional architecture for the generator networks\n",
    "- We use leaky rectified linear units (lRELU) with leaky rate 0.1 ... discrminator networks\n",
    "- normal rectified linear units (RELU) for the generator networks.\n",
    "- Unless noted otherwise, learning rate is 2e-4 for D and 1e-3 for G; $\\lambda$ is set to 1.\n",
    "\n",
    "- For discrete latent codes, we apply a softmax nonlinearity ... For continuous latent codes we parameterize the approximate posterior through a diagonal Gaussian distribution\n",
    "\n",
    "## C.1 MNIST\n",
    "- For this task, we use 1 ten-dimensional categorical code, 2 continuous latent codes and 62 noise variables, resulting in a concatenated dimension of 74.\n",
    "<img src='./1606.03657 InfoGAN/infogan table 1.png'>\n",
    "\n",
    "## C.2 SVHN, C.3 CelebA, C.4 Faces, C.5 Chairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
