{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1312.6114 VAE \n",
    "\n",
    "## Auto-Encoding Variational Bayes\n",
    "\n",
    "- URL : https://arxiv.org/abs/1312.6114\n",
    "- Summary by Song GH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "- __We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions__ ...\n",
    "- Our contributions is two-fold.\n",
    " - First, we show that __a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.__\n",
    " - Second, we show that __for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "- The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior.\n",
    "- __We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound;__\n",
    "- this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference ... with continuous latent variables ... to optimize using standard stochastic gradient ascent techniques\n",
    "\n",
    "- For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the __Auto-Encoding VB (AEVB) algorithm.__\n",
    "- ... we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model ... __without the need of expensive iterative inference schemes (such as MCMC) per datapoint__\n",
    "- The learned __approximate posterior inference model can also be used__ for a host of tasks such as __recognition, denoising, representation and visualization purposes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
